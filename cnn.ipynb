{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#For printing statastics\n",
    "import logging\n",
    "#To interact with OS and System\n",
    "import os \n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(object):\n",
    "    \n",
    "    def __init__(self, learning_rate, num_epochs, beta, batch_size):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.beta = beta\n",
    "        self.batch_size = batch_size\n",
    "        self.save_dir = \"saves\"\n",
    "        self.logs_dir = \"logs\"\n",
    "        os.makedirs(self.save_dir, exist_ok = True)\n",
    "        os.makedirs(self.logs_dir, exist_ok = True)\n",
    "        self.save_path = os.path.join(self.save_dir, \"simple_cnn\")\n",
    "        self.logs_path = os.path.join(self.logs_dir, \"simple_cnn\")\n",
    "        \n",
    "    def build(self, input_tensor, num_classes):\n",
    "        \n",
    "        with tf.name_scope(\"input_placeholders\"):\n",
    "            self.is_training = tf.placeholder_with_default(True, shape=(), name = \"is_training\")\n",
    "            \n",
    "        with tf.name_scope(\"convolutional_layers\"):\n",
    "            conv_1a = tf.layers.conv2d(\n",
    "                input_tensor,\n",
    "                filters = 16,\n",
    "                kernel_size = (1,3),\n",
    "                strides = (1,1),\n",
    "                padding=\"SAME\",\n",
    "                activation = tf.nn.relu,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name = \"conv_1a\"\n",
    "            )\n",
    "            conv_1b = tf.layers.conv2d(\n",
    "                conv_1a,\n",
    "                filters = 16,\n",
    "                kernel_size = (3,1),\n",
    "                strides = (1,1),\n",
    "                padding=\"SAME\",\n",
    "                activation = tf.nn.relu,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name = \"conv_1b\"\n",
    "            )\n",
    "            conv_2a = tf.layers.conv2d(\n",
    "                conv_1b,\n",
    "                filters=32,\n",
    "                kernel_size = (1,3),\n",
    "                strides = (1,1),\n",
    "                padding = \"SAME\",\n",
    "                activation = tf.nn.relu,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name = \"conv_2a\"\n",
    "            )\n",
    "            conv_2b = tf.layers.conv2d(\n",
    "                conv_2a,\n",
    "                filters=32,\n",
    "                kernel_size = (3,1),\n",
    "                strides = (1,1),\n",
    "                padding = \"SAME\",\n",
    "                activation = tf.nn.relu,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name = \"conv_2b\"\n",
    "            )\n",
    "            pool_3a = tf.layers.max_pooling2d(\n",
    "                conv_2b, \n",
    "                pool_size = (1,2),\n",
    "                strides = 1,\n",
    "                padding = \"SAME\",\n",
    "                name = \"pool_3a\"\n",
    "            )\n",
    "            pool_3b = tf.layers.max_pooling2d(\n",
    "                pool_3a, \n",
    "                pool_size = (2,1),\n",
    "                strides = 1,\n",
    "                padding = \"SAME\",\n",
    "                name = \"pool_3b\"\n",
    "            )\n",
    "            drop_4 = tf.layers.dropout(\n",
    "                pool_3b, \n",
    "                training=self.is_training, \n",
    "                name = \"drop_4\"\n",
    "            )\n",
    "            conv_5a = tf.layers.conv2d(\n",
    "                drop_4,\n",
    "                filters = 64,\n",
    "                kernel_size = (1,3),\n",
    "                strides = (1,1),\n",
    "                padding = \"SAME\",\n",
    "                activation = tf.nn.relu,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name = \"conv_5a\"\n",
    "            )\n",
    "            conv_5b = tf.layers.conv2d(\n",
    "                conv_5a,\n",
    "                filters = 64,\n",
    "                kernel_size = (3,1),\n",
    "                strides = (1,1),\n",
    "                padding = \"SAME\",\n",
    "                activation = tf.nn.relu,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name = \"conv_5b\"\n",
    "            )\n",
    "            conv_6a = tf.layers.conv2d(\n",
    "                conv_5b,\n",
    "                filters = 128,\n",
    "                kernel_size = (1,3),\n",
    "                strides = (1,1),\n",
    "                padding = \"SAME\",\n",
    "                activation = tf.nn.relu,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name = \"conv_6a\"\n",
    "            )\n",
    "            conv_6b = tf.layers.conv2d(\n",
    "                conv_6a,\n",
    "                filters = 128,\n",
    "                kernel_size = (3,1),\n",
    "                strides = (1,1),\n",
    "                padding = \"SAME\",\n",
    "                activation = tf.nn.relu,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name = \"conv_6b\"\n",
    "            )\n",
    "            pool_7a = tf.layers.max_pooling2d(\n",
    "                conv_6b,\n",
    "                pool_size = (1,2),\n",
    "                strides = 1,\n",
    "                padding = \"SAME\",\n",
    "                name = \"pool_7a\"\n",
    "            )\n",
    "            pool_7b = tf.layers.max_pooling2d(\n",
    "                pool_7a,\n",
    "                pool_size = (2,1),\n",
    "                strides = 1,\n",
    "                padding = \"SAME\",\n",
    "                name = \"pool_7b\"\n",
    "            )\n",
    "            drop_8 = tf.layers.dropout(\n",
    "                pool_7b,\n",
    "                training=self.is_training,\n",
    "                name = \"drop_8\"\n",
    "            ) \n",
    "        \n",
    "        with tf.name_scope(\"fully_conected_layers\"):\n",
    "            flattened = tf.layers.flatten(\n",
    "                drop_8, \n",
    "                name = \"flatten\"\n",
    "            )\n",
    "            fc_9 = tf.layers.dense(\n",
    "                flattened,\n",
    "                units = 1024,\n",
    "                activation = tf.nn.relu,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name=\"fc_9\"\n",
    "            )\n",
    "            drop_10 = tf.layers.dropout(\n",
    "                fc_9,\n",
    "                training = self.is_training,\n",
    "                name = \"drop_10\"\n",
    "            )\n",
    "            logits = tf.layers.dense(\n",
    "                drop_10,\n",
    "                units = num_classes,\n",
    "                kernel_regularizer = tf.contrib.layers.l2_regularizer(scale = self.beta),\n",
    "                name = \"logits\"\n",
    "            )\n",
    "            \n",
    "        return logits\n",
    "\n",
    "    def _create_tf_dataset(self, x, y):\n",
    "        dataset = tf.data.Dataset.zip((\n",
    "                tf.data.Dataset.from_tensor_slices(x),\n",
    "                tf.data.Dataset.from_tensor_slices(y)\n",
    "        )).shuffle(50).repeat().batch(self.batch_size)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def _log_loss_and_acc(self, epoch, loss, acc, suffix):\n",
    "        summary = tf.Summary(value = [\n",
    "            tf.Summary.Value(tag = \"loss_{}\".format(suffix), simple_value = float(loss)),\n",
    "            tf.Summary.Value(tag = \"acc_{}\".format(suffix), simple_value = float(acc))\n",
    "        ])\n",
    "        self.summary_writer.add_summary(summary, epoch)\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            sess = tf.Session()\n",
    "        \n",
    "            train_dataset = self._create_tf_dataset(X_train, y_train)\n",
    "            valid_dataset = self._create_tf_dataset(X_valid, y_valid)\n",
    "\n",
    "            iterator = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "            next_tensor_batch = iterator.get_next()\n",
    "\n",
    "            train_init_ops = iterator.make_initializer(train_dataset)\n",
    "            valid_init_ops = iterator.make_initializer(valid_dataset)\n",
    "\n",
    "            input_tensor, labels = next_tensor_batch\n",
    "\n",
    "            num_classes = y_train.shape[1]\n",
    "\n",
    "            logits = self.build(input_tensor = input_tensor, num_classes = num_classes)\n",
    "            logger.info('Built network')\n",
    "\n",
    "            prediction = tf.nn.softmax(logits, name = \"predictions\")\n",
    "            loss_ops = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = labels, logits = logits), name = \"loss\")\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate = self.learning_rate)\n",
    "            train_ops = optimizer.minimize(loss_ops)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1), name = \"correct\")\n",
    "            accuracy_ops = tf.reduce_mean(tf.cast(correct, tf.float32), name = \"accuracy\")\n",
    "\n",
    "            initializer = tf.global_variables_initializer()\n",
    "\n",
    "            logger.info('Initializer all variables')\n",
    "            sess.run(initializer)\n",
    "            logger.info('Initialized all variable')\n",
    "\n",
    "            sess.run(train_init_ops)\n",
    "            logger.info('Initialized dataset iterator')\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.summary_writer = tf.summary.FileWriter(self.logs_path)\n",
    "\n",
    "            logger.info(\"Training CNN for {} epochs\".format(self.num_epochs))\n",
    "            for epoch_idx in range(1, self.num_epochs + 1):\n",
    "                loss, _, accuracy = sess.run([\n",
    "                    loss_ops, train_ops, accuracy_ops\n",
    "                ])\n",
    "                self._log_loss_and_acc(epoch_idx, loss, accuracy, \"train\")\n",
    "\n",
    "                if epoch_idx % 10 == 0:\n",
    "                    sess.run(valid_init_ops)\n",
    "                    valid_loss, valid_accuracy = sess.run([\n",
    "                        loss_ops, accuracy_ops\n",
    "                    ], feed_dict = {self.is_training: False})\n",
    "                    logger.info(\"=======================> Epoch {}\".format(epoch_idx))\n",
    "                    logger.info(\"\\tTraining accuracy: {:.3f}\".format(accuracy))\n",
    "                    logger.info(\"\\tTraining loss: {:.6f}\".format(loss))\n",
    "                    logger.info(\"\\tValidation accuracy: {:.3f}\".format(valid_accuracy))\n",
    "                    logger.info(\"\\tValidation loss: {:.6f}\".format(valid_loss))\n",
    "                    self._log_loss_and_acc(epoch_idx, valid_loss, valid_accuracy, \"valid\")\n",
    "\n",
    "                self.saver.save(sess, self.save_path)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-21 14:22:34,312 __main__     INFO     Loading Fashion MNIST data\n",
      "2019-06-21 14:22:34,675 __main__     INFO     Shape of training data:\n",
      "2019-06-21 14:22:34,676 __main__     INFO     Train: (60000, 28, 28)\n",
      "2019-06-21 14:22:34,676 __main__     INFO     Test: (10000, 28, 28)\n",
      "2019-06-21 14:22:34,676 __main__     INFO     Adding channel axis to the data\n",
      "2019-06-21 14:22:34,677 __main__     INFO     Simple transformation by dividing pixels by 255\n",
      "2019-06-21 14:22:34,980 __main__     INFO     Turning ys into one-hot encoding\n",
      "2019-06-21 14:22:34,982 __main__     INFO     Initializing CNN\n",
      "2019-06-21 14:22:34,983 __main__     INFO     Training CNN\n",
      "2019-06-21 14:22:36,070 __main__     INFO     Built network\n",
      "2019-06-21 14:22:36,509 __main__     INFO     Initializer all variables\n",
      "2019-06-21 14:22:38,130 __main__     INFO     Initialized all variable\n",
      "2019-06-21 14:22:38,628 __main__     INFO     Initialized dataset iterator\n",
      "2019-06-21 14:22:38,675 __main__     INFO     Training CNN for 200 epochs\n",
      "2019-06-21 14:23:33,474 __main__     INFO     =======================> Epoch 10\n",
      "2019-06-21 14:23:33,484 __main__     INFO     \tTraining accuracy: 0.250\n",
      "2019-06-21 14:23:33,485 __main__     INFO     \tTraining loss: 1.747078\n",
      "2019-06-21 14:23:33,485 __main__     INFO     \tValidation accuracy: 0.562\n",
      "2019-06-21 14:23:33,485 __main__     INFO     \tValidation loss: 1.347121\n",
      "2019-06-21 14:24:35,694 __main__     INFO     =======================> Epoch 20\n",
      "2019-06-21 14:24:35,704 __main__     INFO     \tTraining accuracy: 0.500\n",
      "2019-06-21 14:24:35,705 __main__     INFO     \tTraining loss: 1.161041\n",
      "2019-06-21 14:24:35,706 __main__     INFO     \tValidation accuracy: 0.594\n",
      "2019-06-21 14:24:35,706 __main__     INFO     \tValidation loss: 1.109546\n",
      "2019-06-21 14:25:36,542 __main__     INFO     =======================> Epoch 30\n",
      "2019-06-21 14:25:36,552 __main__     INFO     \tTraining accuracy: 0.656\n",
      "2019-06-21 14:25:36,552 __main__     INFO     \tTraining loss: 1.064658\n",
      "2019-06-21 14:25:36,553 __main__     INFO     \tValidation accuracy: 0.625\n",
      "2019-06-21 14:25:36,553 __main__     INFO     \tValidation loss: 0.945955\n",
      "2019-06-21 14:26:38,399 __main__     INFO     =======================> Epoch 40\n",
      "2019-06-21 14:26:38,411 __main__     INFO     \tTraining accuracy: 0.656\n",
      "2019-06-21 14:26:38,412 __main__     INFO     \tTraining loss: 0.784774\n",
      "2019-06-21 14:26:38,415 __main__     INFO     \tValidation accuracy: 0.656\n",
      "2019-06-21 14:26:38,417 __main__     INFO     \tValidation loss: 0.784588\n",
      "2019-06-21 14:27:38,052 __main__     INFO     =======================> Epoch 50\n",
      "2019-06-21 14:27:38,055 __main__     INFO     \tTraining accuracy: 0.750\n",
      "2019-06-21 14:27:38,056 __main__     INFO     \tTraining loss: 1.032297\n",
      "2019-06-21 14:27:38,056 __main__     INFO     \tValidation accuracy: 0.688\n",
      "2019-06-21 14:27:38,056 __main__     INFO     \tValidation loss: 0.754803\n",
      "2019-06-21 14:28:39,354 __main__     INFO     =======================> Epoch 60\n",
      "2019-06-21 14:28:39,366 __main__     INFO     \tTraining accuracy: 0.719\n",
      "2019-06-21 14:28:39,367 __main__     INFO     \tTraining loss: 0.740899\n",
      "2019-06-21 14:28:39,368 __main__     INFO     \tValidation accuracy: 0.594\n",
      "2019-06-21 14:28:39,369 __main__     INFO     \tValidation loss: 1.034648\n",
      "2019-06-21 14:29:38,052 __main__     INFO     =======================> Epoch 70\n",
      "2019-06-21 14:29:38,055 __main__     INFO     \tTraining accuracy: 0.750\n",
      "2019-06-21 14:29:38,056 __main__     INFO     \tTraining loss: 0.663821\n",
      "2019-06-21 14:29:38,056 __main__     INFO     \tValidation accuracy: 0.719\n",
      "2019-06-21 14:29:38,056 __main__     INFO     \tValidation loss: 0.508563\n",
      "2019-06-21 14:30:38,825 __main__     INFO     =======================> Epoch 80\n",
      "2019-06-21 14:30:38,826 __main__     INFO     \tTraining accuracy: 0.781\n",
      "2019-06-21 14:30:38,826 __main__     INFO     \tTraining loss: 0.572761\n",
      "2019-06-21 14:30:38,827 __main__     INFO     \tValidation accuracy: 0.750\n",
      "2019-06-21 14:30:38,828 __main__     INFO     \tValidation loss: 0.737682\n",
      "2019-06-21 14:31:36,555 __main__     INFO     =======================> Epoch 90\n",
      "2019-06-21 14:31:36,555 __main__     INFO     \tTraining accuracy: 0.750\n",
      "2019-06-21 14:31:36,556 __main__     INFO     \tTraining loss: 0.734577\n",
      "2019-06-21 14:31:36,556 __main__     INFO     \tValidation accuracy: 0.812\n",
      "2019-06-21 14:31:36,556 __main__     INFO     \tValidation loss: 0.563831\n",
      "2019-06-21 14:32:37,807 __main__     INFO     =======================> Epoch 100\n",
      "2019-06-21 14:32:37,817 __main__     INFO     \tTraining accuracy: 0.719\n",
      "2019-06-21 14:32:37,818 __main__     INFO     \tTraining loss: 0.583669\n",
      "2019-06-21 14:32:37,818 __main__     INFO     \tValidation accuracy: 0.906\n",
      "2019-06-21 14:32:37,819 __main__     INFO     \tValidation loss: 0.340823\n",
      "2019-06-21 14:33:36,595 __main__     INFO     =======================> Epoch 110\n",
      "2019-06-21 14:33:36,604 __main__     INFO     \tTraining accuracy: 0.875\n",
      "2019-06-21 14:33:36,604 __main__     INFO     \tTraining loss: 0.364676\n",
      "2019-06-21 14:33:36,605 __main__     INFO     \tValidation accuracy: 0.844\n",
      "2019-06-21 14:33:36,605 __main__     INFO     \tValidation loss: 0.397932\n",
      "2019-06-21 14:34:41,469 __main__     INFO     =======================> Epoch 120\n",
      "2019-06-21 14:34:41,478 __main__     INFO     \tTraining accuracy: 0.781\n",
      "2019-06-21 14:34:41,479 __main__     INFO     \tTraining loss: 0.510345\n",
      "2019-06-21 14:34:41,480 __main__     INFO     \tValidation accuracy: 0.844\n",
      "2019-06-21 14:34:41,480 __main__     INFO     \tValidation loss: 0.384849\n",
      "2019-06-21 14:35:41,316 __main__     INFO     =======================> Epoch 130\n",
      "2019-06-21 14:35:41,319 __main__     INFO     \tTraining accuracy: 0.812\n",
      "2019-06-21 14:35:41,319 __main__     INFO     \tTraining loss: 0.400545\n",
      "2019-06-21 14:35:41,320 __main__     INFO     \tValidation accuracy: 0.875\n",
      "2019-06-21 14:35:41,320 __main__     INFO     \tValidation loss: 0.314265\n",
      "2019-06-21 14:36:45,588 __main__     INFO     =======================> Epoch 140\n",
      "2019-06-21 14:36:45,598 __main__     INFO     \tTraining accuracy: 0.906\n",
      "2019-06-21 14:36:45,598 __main__     INFO     \tTraining loss: 0.194671\n",
      "2019-06-21 14:36:45,599 __main__     INFO     \tValidation accuracy: 0.875\n",
      "2019-06-21 14:36:45,599 __main__     INFO     \tValidation loss: 0.327681\n",
      "2019-06-21 14:37:45,413 __main__     INFO     =======================> Epoch 150\n",
      "2019-06-21 14:37:45,414 __main__     INFO     \tTraining accuracy: 0.812\n",
      "2019-06-21 14:37:45,414 __main__     INFO     \tTraining loss: 0.665266\n",
      "2019-06-21 14:37:45,415 __main__     INFO     \tValidation accuracy: 0.844\n",
      "2019-06-21 14:37:45,417 __main__     INFO     \tValidation loss: 0.439767\n",
      "2019-06-21 14:38:46,455 __main__     INFO     =======================> Epoch 160\n",
      "2019-06-21 14:38:46,455 __main__     INFO     \tTraining accuracy: 0.844\n",
      "2019-06-21 14:38:46,456 __main__     INFO     \tTraining loss: 0.354759\n",
      "2019-06-21 14:38:46,456 __main__     INFO     \tValidation accuracy: 0.844\n",
      "2019-06-21 14:38:46,458 __main__     INFO     \tValidation loss: 0.314501\n",
      "2019-06-21 14:39:43,912 __main__     INFO     =======================> Epoch 170\n",
      "2019-06-21 14:39:43,913 __main__     INFO     \tTraining accuracy: 0.969\n",
      "2019-06-21 14:39:43,913 __main__     INFO     \tTraining loss: 0.171662\n",
      "2019-06-21 14:39:43,914 __main__     INFO     \tValidation accuracy: 0.875\n",
      "2019-06-21 14:39:43,914 __main__     INFO     \tValidation loss: 0.207018\n",
      "2019-06-21 14:40:44,100 __main__     INFO     =======================> Epoch 180\n",
      "2019-06-21 14:40:44,100 __main__     INFO     \tTraining accuracy: 0.906\n",
      "2019-06-21 14:40:44,101 __main__     INFO     \tTraining loss: 0.209206\n",
      "2019-06-21 14:40:44,101 __main__     INFO     \tValidation accuracy: 0.844\n",
      "2019-06-21 14:40:44,102 __main__     INFO     \tValidation loss: 0.289360\n",
      "2019-06-21 14:41:46,833 __main__     INFO     =======================> Epoch 190\n",
      "2019-06-21 14:41:46,834 __main__     INFO     \tTraining accuracy: 0.875\n",
      "2019-06-21 14:41:46,834 __main__     INFO     \tTraining loss: 0.256347\n",
      "2019-06-21 14:41:46,834 __main__     INFO     \tValidation accuracy: 1.000\n",
      "2019-06-21 14:41:46,835 __main__     INFO     \tValidation loss: 0.059370\n",
      "2019-06-21 14:42:50,767 __main__     INFO     =======================> Epoch 200\n",
      "2019-06-21 14:42:50,767 __main__     INFO     \tTraining accuracy: 0.969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-21 14:42:50,768 __main__     INFO     \tTraining loss: 0.092662\n",
      "2019-06-21 14:42:50,768 __main__     INFO     \tValidation accuracy: 0.969\n",
      "2019-06-21 14:42:50,768 __main__     INFO     \tValidation loss: 0.166136\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(stream = sys.stdout,\n",
    "                       level = logging.DEBUG,\n",
    "                       format = '%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    logger.info(\"Loading Fashion MNIST data\")\n",
    "    (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    logger.info('Shape of training data:')\n",
    "    logger.info('Train: {}'.format(X_train.shape))\n",
    "    logger.info('Test: {}'.format(X_test.shape))\n",
    "    \n",
    "    logger.info('Adding channel axis to the data')\n",
    "    X_train = X_train[:, :, :, np.newaxis]\n",
    "    X_test = X_test[:, :, :, np.newaxis]\n",
    "    \n",
    "    logger.info(\"Simple transformation by dividing pixels by 255\")\n",
    "    X_train = X_train/255\n",
    "    X_test = X_test/255\n",
    "    \n",
    "    X_train = X_train.astype('f')\n",
    "    X_test = X_test.astype('f')\n",
    "    y_train = y_train.astype('f')\n",
    "    y_test = y_test.astype('f')\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    logger.info(\"Turning ys into one-hot encoding\")\n",
    "    y_train = np_utils.to_categorical(y_train, num_classes = num_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, num_classes = num_classes)\n",
    "    \n",
    "    cnn_params = {\n",
    "        \"learning_rate\": 3e-4,\n",
    "        \"num_epochs\": 200,\n",
    "        \"beta\": 1e-3,\n",
    "        \"batch_size\": 32\n",
    "    }\n",
    "    \n",
    "    logger.info('Initializing CNN')\n",
    "    simple_cnn = SimpleCNN(**cnn_params)\n",
    "    logger.info('Training CNN')\n",
    "    simple_cnn.fit(X_train = X_train,\n",
    "                  X_valid = X_test,\n",
    "                  y_train = y_train,\n",
    "                  y_valid = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
